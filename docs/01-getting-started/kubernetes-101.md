# Kubernetes Fundamentals: Architecture and Core Concepts

A comprehensive guide to understanding Kubernetes architecture, from containers to cloud providers.

## Table of Contents
- [What is Kubernetes?](#what-is-kubernetes)
- [Core Concepts](#core-concepts)
- [Cluster Architecture](#cluster-architecture)
- [Control Plane Components](#control-plane-components)
- [Node Components](#node-components)
- [Layered Architecture](#layered-architecture)
- [Cloud Provider Integration](#cloud-provider-integration)
- [EKS: Managed Kubernetes on AWS](#eks-managed-kubernetes-on-aws)
- [How It All Works Together](#how-it-all-works-together)
- [Real-World Example from This Project](#real-world-example-from-this-project)

---

## What is Kubernetes?

**Kubernetes (K8s)** is an open-source container orchestration platform that automates deployment, scaling, and management of containerized applications.

### The Problem Kubernetes Solves

**Without Kubernetes:**
```
Developer â†’ Manual deployment on servers
           â†’ Manual scaling when traffic increases
           â†’ Manual recovery when containers crash
           â†’ Manual load balancing
           â†’ Manual updates (downtime required)
```

**With Kubernetes:**
```
Developer â†’ Declare desired state (YAML)
           â†’ K8s automatically deploys containers
           â†’ K8s automatically scales based on load
           â†’ K8s automatically restarts failed containers
           â†’ K8s automatically load balances traffic
           â†’ K8s performs rolling updates (zero downtime)
```

### Key Philosophy

Kubernetes uses a **declarative model**:
- **You declare** what you want (desired state)
- **Kubernetes ensures** that's what you get (actual state)
- **Control loops** constantly reconcile desired vs actual

```
Desired State (YAML) â†’ K8s Controllers â†’ Actual State (Running Pods)
         â†‘                                        â†“
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Reconciliation Loop â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Core Concepts

### 1. Containers

**What:** Lightweight, standalone packages containing application code and dependencies.

**Example:**
```dockerfile
# Docker container for our Day service
FROM python:3.11-slim
COPY app.py /app/
RUN pip install flask
CMD ["python", "/app/app.py"]
```

**Kubernetes manages containers, but doesn't run them directly.**

### 2. Pods

**The smallest deployable unit in Kubernetes.** A Pod wraps one or more containers.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Pod: day-7d4f9c8b5f-abc12   â”‚
â”‚                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Container: day        â”‚  â”‚
â”‚  â”‚ Image: day:v1.2.3     â”‚  â”‚
â”‚  â”‚ Port: 8001            â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                             â”‚
â”‚  IP: 10.0.1.45              â”‚
â”‚  Status: Running            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Why Pods, not just containers?**
- Pods share network namespace (localhost communication)
- Pods share storage volumes
- Pods are scheduled together on same node
- Pods are the unit of scaling

**Common pattern:** 1 Pod = 1 main container (+ optional sidecar containers)

### 3. Deployments

**Manages the lifecycle of Pods.** Handles scaling, updates, and rollbacks.

```
Deployment: day-service (desired: 3 replicas)
    â†“ creates
ReplicaSet: day-7d4f9c8b5f (actual: 3 replicas)
    â†“ creates
Pods:
    - day-7d4f9c8b5f-abc12 (Running)
    - day-7d4f9c8b5f-def34 (Running)
    - day-7d4f9c8b5f-ghi56 (Running)
```

**What Deployments provide:**
- âœ… Declarative updates (change image â†’ rolling update)
- âœ… Scaling (change replicas: 3 â†’ 10)
- âœ… Rollback (revert to previous version)
- âœ… Self-healing (pod crashes â†’ automatically replaced)

See [deployment-hierarchy.md](deployment-hierarchy.md) for deep dive.

### 4. Services

**Stable network endpoint** for accessing Pods.

**Problem:** Pods have dynamic IPs that change when they're recreated.

**Solution:** Services provide a stable IP/DNS name.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Service: day-service             â”‚
â”‚ Type: ClusterIP                  â”‚
â”‚ IP: 10.100.200.50 (stable!)      â”‚
â”‚ DNS: day-service.production.svc  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚ routes traffic to
              â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â†“                   â†“
Pod: 10.0.1.45      Pod: 10.0.1.67
(day-abc12)         (day-def34)
```

**Service types:**
- **ClusterIP** - Internal-only (default)
- **NodePort** - Exposes on each node's IP
- **LoadBalancer** - Cloud load balancer (AWS ALB/NLB)

### 5. Namespaces

**Virtual clusters** within a physical cluster.

```
Cluster: day-cluster
â”œâ”€â”€ Namespace: production
â”‚   â”œâ”€â”€ Deployment: day-service
â”‚   â”œâ”€â”€ Service: day-service
â”‚   â””â”€â”€ ConfigMap: day-config
â”œâ”€â”€ Namespace: dev
â”‚   â”œâ”€â”€ Deployment: day-service (different version)
â”‚   â”œâ”€â”€ Service: day-service
â”‚   â””â”€â”€ ConfigMap: day-config (different settings)
â””â”€â”€ Namespace: kube-system
    â”œâ”€â”€ CoreDNS
    â”œâ”€â”€ ALB Controller
    â””â”€â”€ Metrics Server
```

**Why namespaces?**
- Resource isolation
- Access control (RBAC per namespace)
- Resource quotas
- Logical separation (teams, environments)

### 6. ConfigMaps and Secrets

**ConfigMaps** - Store non-sensitive configuration
**Secrets** - Store sensitive data (base64 encoded)

```yaml
# ConfigMap
apiVersion: v1
kind: ConfigMap
metadata:
  name: day-config
data:
  LOG_LEVEL: "info"
  PORT: "8001"
---
# Secret
apiVersion: v1
kind: Secret
metadata:
  name: db-credentials
type: Opaque
data:
  username: YWRtaW4=  # base64 encoded
  password: cGFzc3dvcmQ=
```

**How Pods use them:**
- Environment variables
- Mounted as files

See [configmap-relationships.md](configmap-relationships.md) for details.

### 7. Ingress

**HTTP/HTTPS routing** to Services.

```
                   Internet
                      â†“
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ AWS ALB       â”‚ â† Created by Ingress Controller
              â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ Ingress       â”‚ â† Kubernetes resource
              â”‚ Rules:        â”‚
              â”‚ - /api â†’ api  â”‚
              â”‚ - /web â†’ web  â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â†“                         â†“
    Service: api             Service: web
         â†“                         â†“
    Pods: api-*             Pods: web-*
```

**Ingress Controller** (runs in cluster) reads Ingress resources and configures load balancer.

### 8. Volumes

**Persistent storage** that survives Pod restarts.

```
PersistentVolumeClaim (PVC)
    â†“ binds to
PersistentVolume (PV)
    â†“ backed by
AWS EBS Volume / EFS / S3
```

**Volume types:**
- **emptyDir** - Temporary, deleted with Pod
- **configMap/secret** - Mount config as files
- **persistentVolumeClaim** - Persistent storage
- **hostPath** - Node's filesystem (testing only)

---

## Cluster Architecture

Kubernetes cluster = **Control Plane** + **Worker Nodes**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    KUBERNETES CLUSTER                       â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚         CONTROL PLANE (Master Nodes)                â”‚   â”‚
â”‚  â”‚                                                     â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚   â”‚
â”‚  â”‚  â”‚ API      â”‚  â”‚Scheduler â”‚  â”‚Controllerâ”‚         â”‚   â”‚
â”‚  â”‚  â”‚ Server   â”‚  â”‚          â”‚  â”‚ Manager  â”‚         â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚   â”‚
â”‚  â”‚                                                     â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚   â”‚
â”‚  â”‚  â”‚         etcd (cluster store)        â”‚           â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                          â”‚                                  â”‚
â”‚                          â”‚ manages                          â”‚
â”‚                          â†“                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              WORKER NODES (Data Plane)              â”‚   â”‚
â”‚  â”‚                                                     â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚   â”‚
â”‚  â”‚  â”‚ Node 1          â”‚  â”‚ Node 2          â”‚         â”‚   â”‚
â”‚  â”‚  â”‚                 â”‚  â”‚                 â”‚         â”‚   â”‚
â”‚  â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”‚         â”‚   â”‚
â”‚  â”‚  â”‚ â”‚Pod 1â”‚ â”‚Pod 2â”‚ â”‚  â”‚ â”‚Pod 3â”‚ â”‚Pod 4â”‚ â”‚   ...   â”‚   â”‚
â”‚  â”‚  â”‚ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â”‚  â”‚ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â”‚         â”‚   â”‚
â”‚  â”‚  â”‚                 â”‚  â”‚                 â”‚         â”‚   â”‚
â”‚  â”‚  â”‚ kubelet         â”‚  â”‚ kubelet         â”‚         â”‚   â”‚
â”‚  â”‚  â”‚ kube-proxy      â”‚  â”‚ kube-proxy      â”‚         â”‚   â”‚
â”‚  â”‚  â”‚ Container       â”‚  â”‚ Container       â”‚         â”‚   â”‚
â”‚  â”‚  â”‚ Runtime         â”‚  â”‚ Runtime         â”‚         â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Control Plane vs Data Plane

**Control Plane (Brain)**
- Makes decisions
- Stores cluster state
- Schedules workloads
- Responds to events

**Data Plane (Muscle)**
- Runs application containers
- Executes control plane decisions
- Reports status back

**In managed services (EKS):** AWS manages control plane, you manage nodes.

---

## Control Plane Components

### 1. API Server (kube-apiserver)

**The front door to Kubernetes.** All communication goes through here.

```
kubectl â†’ API Server â†’ etcd
Scheduler â†’ API Server â†’ etcd
Kubelet â†’ API Server â†’ etcd
Controllers â†’ API Server â†’ etcd
```

**What it does:**
- âœ… Validates and processes API requests
- âœ… Authenticates users and service accounts
- âœ… Authorizes actions (RBAC)
- âœ… Persists state to etcd
- âœ… Serves API (REST)

**Example interaction:**
```bash
kubectl apply -f deployment.yaml
    â†“
1. kubectl sends HTTP POST to API Server
2. API Server validates YAML syntax
3. API Server checks authentication (who are you?)
4. API Server checks authorization (can you create Deployments?)
5. API Server validates resource schema
6. API Server writes to etcd
7. API Server returns success to kubectl
```

**In EKS:** AWS manages API Server, you access it via endpoint.

### 2. etcd

**Distributed key-value store** - the database of Kubernetes.

```
etcd stores:
â”œâ”€â”€ All cluster configuration
â”œâ”€â”€ All resource definitions (Deployments, Services, Pods)
â”œâ”€â”€ All resource status (what's running where)
â”œâ”€â”€ Secrets and ConfigMaps
â””â”€â”€ Everything!
```

**Properties:**
- Strongly consistent (Raft consensus)
- Distributed (3-5 replicas for HA)
- Watch API (controllers watch for changes)
- Highly available

**Example data:**
```
Key: /registry/pods/production/day-7d4f9c8b5f-abc12
Value: {
  "metadata": {...},
  "spec": {...},
  "status": {
    "phase": "Running",
    "podIP": "10.0.1.45"
  }
}
```

**In EKS:** AWS fully manages etcd, automatic backups.

### 3. Scheduler (kube-scheduler)

**Assigns Pods to Nodes.**

```
1. Watch for new Pods with no node assignment
2. Filter nodes (which nodes can run this Pod?)
   - Has enough CPU/memory?
   - Matches node selectors?
   - Tolerates node taints?
3. Score nodes (which node is best?)
   - Resource utilization
   - Pod spreading
   - Affinity rules
4. Assign Pod to highest-scoring node
5. Update Pod spec with nodeName
```

**Example:**
```
New Pod created: day-7d4f9c8b5f-abc12
  Requires: 200m CPU, 256Mi memory

Node 1: 1000m CPU available, 2Gi memory â†’ Score: 80
Node 2: 500m CPU available, 1Gi memory â†’ Score: 60
Node 3: 100m CPU available, 512Mi memory â†’ Score: 20

Scheduler assigns to Node 1 (highest score)
```

**Factors considered:**
- Resource requests/limits
- Node affinity/anti-affinity
- Pod affinity/anti-affinity
- Taints and tolerations
- Topology spread constraints

### 4. Controller Manager (kube-controller-manager)

**Runs multiple controllers** that reconcile desired state with actual state.

```
Controller Manager contains:
â”œâ”€â”€ Deployment Controller (manages ReplicaSets)
â”œâ”€â”€ ReplicaSet Controller (manages Pods)
â”œâ”€â”€ Node Controller (monitors node health)
â”œâ”€â”€ Service Account Controller (creates default accounts)
â”œâ”€â”€ Endpoint Controller (populates Service endpoints)
â”œâ”€â”€ Namespace Controller (cleans up deleted namespaces)
â””â”€â”€ ... many more
```

**How controllers work:**
```
Loop forever:
1. Watch for changes (via API Server)
2. Compare desired state vs actual state
3. Take action to reconcile
4. Wait for next change
```

**Example - Deployment Controller:**
```
Event: Deployment updated (replicas: 2 â†’ 5)
    â†“
1. Deployment Controller sees change
2. Checks current ReplicaSet (2 pods)
3. Needs 5 pods â†’ creates 3 more
4. ReplicaSet Controller sees change
5. Creates 3 new Pods
6. Scheduler assigns to nodes
7. Kubelets start containers
```

See [deployment-hierarchy.md](deployment-hierarchy.md) for details.

### 5. Cloud Controller Manager (cloud-controller-manager)

**Interfaces with cloud provider APIs** (AWS, GCP, Azure).

```
Cloud Controller Manager contains:
â”œâ”€â”€ Node Controller (registers EC2 instances as nodes)
â”œâ”€â”€ Route Controller (sets up networking routes)
â”œâ”€â”€ Service Controller (creates LoadBalancers)
â””â”€â”€ Volume Controller (provisions EBS volumes)
```

**AWS-specific examples:**
- Creates AWS ALB when you create LoadBalancer Service
- Provisions EBS volumes for PersistentVolumeClaims
- Adds EC2 instance metadata to Nodes
- Configures VPC routing

More details in [Cloud Provider Integration](#cloud-provider-integration) section.

---

## Node Components

Worker nodes run the actual application containers.

### 1. Kubelet

**The node agent.** Runs on every node and manages Pods.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Worker Node              â”‚
â”‚                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚       Kubelet              â”‚  â”‚
â”‚  â”‚                            â”‚  â”‚
â”‚  â”‚  1. Watch API Server       â”‚  â”‚
â”‚  â”‚     for Pods assigned      â”‚  â”‚
â”‚  â”‚     to this node           â”‚  â”‚
â”‚  â”‚                            â”‚  â”‚
â”‚  â”‚  2. Tell Container Runtime â”‚  â”‚
â”‚  â”‚     to start/stop          â”‚  â”‚
â”‚  â”‚     containers             â”‚  â”‚
â”‚  â”‚                            â”‚  â”‚
â”‚  â”‚  3. Monitor container      â”‚  â”‚
â”‚  â”‚     health (probes)        â”‚  â”‚
â”‚  â”‚                            â”‚  â”‚
â”‚  â”‚  4. Report status to       â”‚  â”‚
â”‚  â”‚     API Server             â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚              â†“                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Container Runtime        â”‚  â”‚
â”‚  â”‚   (containerd/Docker)      â”‚  â”‚
â”‚  â”‚                            â”‚  â”‚
â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”      â”‚  â”‚
â”‚  â”‚   â”‚Pod 1 â”‚  â”‚Pod 2 â”‚      â”‚  â”‚
â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”˜      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Kubelet responsibilities:**
- âœ… Pull container images
- âœ… Start/stop containers
- âœ… Run liveness/readiness probes
- âœ… Mount volumes
- âœ… Report node/pod status
- âœ… Execute container commands (kubectl exec)
- âœ… Stream logs (kubectl logs)

**Example flow:**
```
1. API Server: "Pod abc12 scheduled to node-1"
2. Kubelet on node-1: "I see new Pod assigned to me"
3. Kubelet: "Pull image: day:v1.2.3"
4. Kubelet â†’ Container Runtime: "Start container"
5. Container Runtime: "Container started"
6. Kubelet: "Wait 10 seconds (initialDelaySeconds)"
7. Kubelet: "Run readiness probe: HTTP GET /health"
8. Kubelet: "Probe succeeded, Pod is Ready"
9. Kubelet â†’ API Server: "Pod abc12 status: Running, Ready"
```

### 2. Container Runtime

**Runs containers.** Kubelet talks to runtime via CRI (Container Runtime Interface).

**Common runtimes:**
- **containerd** (most common, default in EKS)
- **Docker** (via dockershim, deprecated)
- **CRI-O**

```
Kubelet (CRI client)
    â†“ CRI gRPC
Container Runtime (CRI server)
    â†“
Low-level container management
    â†“
Linux kernel (namespaces, cgroups)
    â†“
Running containers
```

**What runtime does:**
- Pull images from registry (ECR, Docker Hub)
- Create container filesystem
- Set up namespaces (network, PID, mount)
- Apply resource limits (cgroups)
- Start container processes
- Monitor container lifecycle

### 3. kube-proxy

**Manages network rules** for Service load balancing.

**Problem:** Services have virtual IPs that don't exist on network.

**Solution:** kube-proxy sets up iptables/ipvs rules to route traffic.

```
Pod tries to connect to: day-service:8001
    â†“
DNS resolves to: 10.100.200.50 (ClusterIP)
    â†“
Packet sent to: 10.100.200.50:8001
    â†“
iptables rule (created by kube-proxy):
  If destination = 10.100.200.50:8001
  Then randomly forward to:
    - 10.0.1.45:8001 (Pod 1)
    - 10.0.1.67:8001 (Pod 2)
    - 10.0.2.12:8001 (Pod 3)
    â†“
Packet arrives at actual Pod IP
```

**kube-proxy modes:**
- **iptables** (default) - Uses Linux iptables rules
- **ipvs** - Uses Linux IPVS (better performance, more modes)
- **userspace** (legacy) - Proxies traffic in userspace

**In EKS:** Uses iptables mode by default.

---

## Layered Architecture

Kubernetes architecture can be understood as layers of abstraction.

### Layer 1: Infrastructure Layer

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Infrastructure (Cloud/Bare Metal)      â”‚
â”‚                                          â”‚
â”‚   AWS: EC2, VPC, EBS, IAM               â”‚
â”‚   GCP: Compute Engine, VPC, Disks       â”‚
â”‚   On-prem: Physical servers, storage    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Managed by:** Cloud provider or ops team
**Kubernetes sees:** Compute, network, storage primitives

### Layer 2: Cluster Layer

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Kubernetes Cluster                  â”‚
â”‚                                          â”‚
â”‚   Control Plane + Worker Nodes          â”‚
â”‚   Networking (CNI)                       â”‚
â”‚   Storage (CSI)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Managed by:** Platform team (or AWS in EKS)
**Provides:** Container orchestration, scheduling, networking

### Layer 3: Platform Layer

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Platform Services (Add-ons)            â”‚
â”‚                                          â”‚
â”‚   Ingress Controller (ALB/NGINX)        â”‚
â”‚   Monitoring (Prometheus)                â”‚
â”‚   Logging (Fluentd)                      â”‚
â”‚   Service Mesh (Istio) - optional        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Managed by:** Platform team
**Provides:** Shared services for applications

### Layer 4: Application Layer

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Applications                      â”‚
â”‚                                          â”‚
â”‚   Deployments, Services, ConfigMaps     â”‚
â”‚   Your microservices                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Managed by:** Application teams
**Provides:** Business logic, user-facing services

### Layer Interaction

```
Application â†’ Platform â†’ Cluster â†’ Infrastructure

Example: Create LoadBalancer Service
    â†“
1. Application Layer: kubectl apply service.yaml
2. Cluster Layer: API Server stores Service
3. Cluster Layer: Service Controller sees new Service
4. Cluster Layer: Cloud Controller Manager called
5. Infrastructure Layer: AWS API creates ALB
6. Infrastructure Layer: ALB configured with targets
7. Platform Layer: Ingress Controller updates rules
8. Application Layer: Traffic flows to Pods
```

---

## Cloud Provider Integration

Kubernetes interfaces with cloud providers through **Cloud Controller Manager** and **CSI/CNI plugins**.

### Integration Points

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    KUBERNETES CLUSTER                     â”‚
â”‚                                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚   Cloud Controller Manager                      â”‚     â”‚
â”‚  â”‚                                                 â”‚     â”‚
â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚     â”‚
â”‚  â”‚   â”‚   Node    â”‚  â”‚  Service   â”‚  â”‚  Route   â”‚ â”‚     â”‚
â”‚  â”‚   â”‚Controller â”‚  â”‚ Controller â”‚  â”‚Controllerâ”‚ â”‚     â”‚
â”‚  â”‚   â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚            â”‚                â”‚             â”‚             â”‚
â”‚            â†“                â†“             â†“             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚        Cloud Provider APIs (AWS)                â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚              â”‚              â”‚
         â†“              â†“              â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  EC2   â”‚     â”‚  ALB   â”‚    â”‚  VPC   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1. Node Integration

**Kubernetes discovers and registers cloud instances as nodes.**

**AWS Example:**
```
1. EC2 instance starts with kubelet
2. Kubelet registers with API Server
3. Cloud Controller Manager:
   - Queries EC2 API for instance metadata
   - Adds labels: instance-type, availability-zone, region
   - Adds annotations: instance-id, public-ip
   - Monitors instance status
4. Node appears in kubectl get nodes
```

**Node labels from AWS:**
```yaml
metadata:
  labels:
    node.kubernetes.io/instance-type: t3.small
    topology.kubernetes.io/region: us-east-1
    topology.kubernetes.io/zone: us-east-1a
    eks.amazonaws.com/nodegroup: day-nodes
```

### 2. Load Balancer Integration

**Kubernetes Services create cloud load balancers.**

**AWS Example - LoadBalancer Service:**
```yaml
apiVersion: v1
kind: Service
metadata:
  name: day-service
spec:
  type: LoadBalancer  # â† Triggers AWS integration
  selector:
    app: day
  ports:
  - port: 80
    targetPort: 8001
```

**What happens:**
```
1. Service created in Kubernetes
2. Service Controller sees type: LoadBalancer
3. Cloud Controller Manager:
   - Calls AWS API: CreateLoadBalancer
   - Creates Classic ELB or NLB
   - Configures health checks
   - Registers node IPs as targets
   - Waits for LB to become active
4. Updates Service with external IP
5. kubectl get svc shows EXTERNAL-IP: abc.us-east-1.elb.amazonaws.com
```

**AWS Ingress (ALB) via Ingress Controller:**
```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: day-ingress
  annotations:
    alb.ingress.kubernetes.io/scheme: internet-facing
spec:
  ingressClassName: alb
  rules:
  - host: day.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: day-service
            port:
              number: 80
```

**AWS Load Balancer Controller:**
```
1. Watches for Ingress resources
2. Calls AWS API:
   - CreateLoadBalancer (ALB)
   - CreateTargetGroup
   - CreateListener
   - RegisterTargets (Pod IPs directly!)
3. Configures routing rules
4. Manages ALB lifecycle
```

### 3. Storage Integration (CSI)

**Container Storage Interface** - standard for storage plugins.

**AWS EBS CSI Driver:**
```
Kubernetes StorageClass
    â†“
PersistentVolumeClaim created
    â†“
CSI Controller:
  - Calls AWS API: CreateVolume (EBS)
  - Waits for volume creation
  - Creates PersistentVolume in K8s
    â†“
Pod scheduled to node
    â†“
CSI Node Plugin:
  - Calls AWS API: AttachVolume
  - Mounts volume to node
  - Mounts into container
    â†“
Container has persistent storage
```

**Example:**
```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: ebs-claim
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
  storageClassName: gp3  # â† AWS EBS gp3 type
```

**Behind the scenes:**
```bash
# AWS creates:
aws ec2 create-volume --size 10 --volume-type gp3

# Attaches to node:
aws ec2 attach-volume --volume-id vol-xxx --instance-id i-yyy

# Kubelet mounts:
mount /dev/xvdf /var/lib/kubelet/pods/.../volumes/ebs-claim
```

### 4. Networking Integration (CNI)

**Container Network Interface** - standard for network plugins.

**AWS VPC CNI:**
```
Pod created
    â†“
CNI Plugin:
  - Allocates ENI (Elastic Network Interface) to node
  - Assigns secondary IPs to ENI
  - Gives Pod an IP from VPC subnet
  - Pod has real VPC IP (routable!)
    â†“
Pod can communicate:
  - With other Pods (direct VPC routing)
  - With AWS services (S3, RDS, etc.)
  - With on-prem (via VPN/DirectConnect)
```

**Benefit:** Pods are first-class VPC citizens.

### 5. Identity Integration (IRSA)

**IAM Roles for Service Accounts** - gives Pods AWS permissions.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Pod: day-service                  â”‚
â”‚                                    â”‚
â”‚  ServiceAccount: day-sa            â”‚
â”‚  Annotation:                       â”‚
â”‚    eks.amazonaws.com/role-arn:     â”‚
â”‚      arn:aws:iam::xxx:role/DayRole â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
    Pod makes AWS API call
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AWS STS AssumeRoleWithWebIdentity â”‚
â”‚  - Validates OIDC token from EKS   â”‚
â”‚  - Returns temporary credentials   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
    Pod has AWS permissions!
```

**Example use case:**
```python
# Pod can access S3 without hardcoded credentials
import boto3

s3 = boto3.client('s3')  # Automatically uses IRSA credentials
s3.list_buckets()        # Works if IAM role has s3:ListBuckets
```

---

## EKS: Managed Kubernetes on AWS

**Amazon Elastic Kubernetes Service (EKS)** is AWS's managed Kubernetes offering.

### What AWS Manages vs What You Manage

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        AWS MANAGES (Control Plane)              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  âœ… API Server (multi-AZ, auto-scaling)         â”‚
â”‚  âœ… etcd (encrypted, backed up)                 â”‚
â”‚  âœ… Scheduler                                   â”‚
â”‚  âœ… Controller Manager                          â”‚
â”‚  âœ… Cloud Controller Manager                    â”‚
â”‚  âœ… Control plane upgrades                      â”‚
â”‚  âœ… Control plane security patches              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        YOU MANAGE (Data Plane)                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  âš™ï¸  Worker Nodes (EC2 instances)               â”‚
â”‚  âš™ï¸  Node Groups / Auto Scaling Groups          â”‚
â”‚  âš™ï¸  Node security patches                      â”‚
â”‚  âš™ï¸  Node upgrades                              â”‚
â”‚  âš™ï¸  Add-ons (ALB Controller, CSI drivers)      â”‚
â”‚  âš™ï¸  Applications and workloads                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### EKS Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AWS ACCOUNT                        â”‚
â”‚                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  EKS Control Plane (AWS-managed VPC)        â”‚     â”‚
â”‚  â”‚                                             â”‚     â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚     â”‚
â”‚  â”‚  â”‚ API      â”‚  â”‚Scheduler â”‚  â”‚   etcd   â”‚  â”‚     â”‚
â”‚  â”‚  â”‚ Server   â”‚  â”‚          â”‚  â”‚          â”‚  â”‚     â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚     â”‚
â”‚  â”‚                                             â”‚     â”‚
â”‚  â”‚  Multi-AZ, Auto-scaled, Highly Available    â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                    â”‚                                 â”‚
â”‚                    â”‚ Secured endpoint                â”‚
â”‚                    â”‚ (public or private)             â”‚
â”‚                    â†“                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚     Your VPC (10.1.0.0/16)                  â”‚    â”‚
â”‚  â”‚                                             â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚    â”‚
â”‚  â”‚  â”‚  Subnet 1   â”‚  â”‚  Subnet 2   â”‚          â”‚    â”‚
â”‚  â”‚  â”‚  us-east-1a â”‚  â”‚  us-east-1b â”‚          â”‚    â”‚
â”‚  â”‚  â”‚             â”‚  â”‚             â”‚          â”‚    â”‚
â”‚  â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚          â”‚    â”‚
â”‚  â”‚  â”‚ â”‚ Node 1  â”‚ â”‚  â”‚ â”‚ Node 2  â”‚ â”‚          â”‚    â”‚
â”‚  â”‚  â”‚ â”‚ (EC2)   â”‚ â”‚  â”‚ â”‚ (EC2)   â”‚ â”‚          â”‚    â”‚
â”‚  â”‚  â”‚ â”‚         â”‚ â”‚  â”‚ â”‚         â”‚ â”‚          â”‚    â”‚
â”‚  â”‚  â”‚ â”‚ Pods    â”‚ â”‚  â”‚ â”‚ Pods    â”‚ â”‚          â”‚    â”‚
â”‚  â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚          â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### How EKS Relates to Kubernetes

**EKS IS Kubernetes** - just with AWS managing the control plane.

**Compatibility:**
- âœ… 100% upstream Kubernetes (certified conformant)
- âœ… Standard kubectl works
- âœ… Standard Kubernetes APIs
- âœ… Standard YAML manifests
- âœ… Portable to other Kubernetes (GKE, AKS, self-managed)

**AWS-specific integrations:**
- VPC CNI (Pods get VPC IPs)
- AWS Load Balancer Controller (ALB/NLB creation)
- EBS CSI Driver (EBS volume provisioning)
- EFS CSI Driver (EFS filesystem mounting)
- IRSA (IAM roles for Pods)

### EKS Cluster Creation Flow

```
1. Create EKS Cluster (via AWS Console, CLI, or Pulumi)
   - AWS creates control plane in AWS-managed VPC
   - Creates API endpoint (public and/or private)
   - Sets up OIDC provider for IRSA
   - Takes ~10-15 minutes

2. Create Node Group (Managed or Self-managed)
   - AWS launches EC2 instances
   - Instances join cluster (via bootstrap script)
   - Kubelet registers with API Server
   - Nodes appear in kubectl get nodes

3. Install Add-ons
   - VPC CNI (networking) - pre-installed
   - kube-proxy - pre-installed
   - CoreDNS - pre-installed
   - AWS Load Balancer Controller - install yourself
   - EBS CSI Driver - install yourself
   - Metrics Server - install yourself

4. Deploy Applications
   - kubectl apply or Helm or ArgoCD
   - Pods scheduled to nodes
   - Services create load balancers
   - Applications running!
```

### Accessing EKS Cluster

**Authentication:**
```bash
# Update kubeconfig with EKS cluster info
aws eks update-kubeconfig --name day-cluster --region us-east-1

# This creates ~/.kube/config entry:
# - Cluster API endpoint
# - Certificate authority data
# - Auth command: aws eks get-token

# When you run kubectl:
kubectl get pods
    â†“
1. kubectl reads ~/.kube/config
2. Runs: aws eks get-token --cluster-name day-cluster
3. AWS returns temporary token (via STS)
4. kubectl sends token to EKS API Server
5. EKS validates token with AWS IAM
6. EKS checks IAM permissions (RBAC)
7. Request processed if authorized
```

**IAM to Kubernetes mapping:**
```
AWS IAM User/Role
    â†“ mapped via aws-auth ConfigMap
Kubernetes User/Group
    â†“ bound via RoleBinding
Kubernetes Role
    â†“ defines
Permissions (verbs on resources)
```

### EKS Networking Deep Dive

**VPC CNI Plugin:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Node (EC2 instance)                 â”‚
â”‚                                      â”‚
â”‚  Primary ENI (eth0)                  â”‚
â”‚  - Primary IP: 10.1.1.50            â”‚
â”‚  - Secondary IPs:                    â”‚
â”‚    - 10.1.1.51 â†’ Pod 1              â”‚
â”‚    - 10.1.1.52 â†’ Pod 2              â”‚
â”‚    - 10.1.1.53 â†’ Pod 3              â”‚
â”‚    - 10.1.1.54 â†’ Pod 4              â”‚
â”‚                                      â”‚
â”‚  Secondary ENI (eth1) if needed      â”‚
â”‚  - Primary IP: 10.1.1.60            â”‚
â”‚  - Secondary IPs:                    â”‚
â”‚    - 10.1.1.61 â†’ Pod 5              â”‚
â”‚    - ...                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Max Pods per node:**
```
Formula: (ENIs Ã— IPs per ENI) - 1
Example for t3.small:
  - Max ENIs: 3
  - IPs per ENI: 4
  - Max Pods: (3 Ã— 4) - 1 = 11
```

**Benefits of VPC CNI:**
- Pods are VPC citizens (Security Groups apply)
- Direct Pod-to-Pod routing (no overlay)
- Pod IPs routable from on-prem
- Simpler network debugging

**Trade-off:** Uses VPC IP addresses (plan subnet size accordingly!)

---

## How It All Works Together

Let's trace a complete flow: deploying an application to EKS.

### Scenario: Deploy Day Service

**1. Developer creates Deployment:**
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: day-service
  namespace: production
spec:
  replicas: 3
  selector:
    matchLabels:
      app: day
  template:
    metadata:
      labels:
        app: day
    spec:
      containers:
      - name: day
        image: 123456789.dkr.ecr.us-east-1.amazonaws.com/day:v1.2.3
        ports:
        - containerPort: 8001
        env:
        - name: LOG_LEVEL
          value: "info"
```

**2. Apply to cluster:**
```bash
kubectl apply -f deployment.yaml
```

**3. API Server processing:**
```
kubectl â†’ HTTPS POST â†’ EKS API Server (AWS-managed)
    â†“
1. API Server authenticates (AWS IAM token)
2. API Server authorizes (RBAC check)
3. API Server validates YAML schema
4. API Server writes to etcd
5. API Server returns: "deployment.apps/day-service created"
```

**4. Deployment Controller processing:**
```
Deployment Controller (in AWS-managed control plane):
1. Watch detects new Deployment
2. Reads: replicas=3, no ReplicaSets exist
3. Creates ReplicaSet: day-service-7d4f9c8b5f
4. Sets replicas=3 on ReplicaSet
5. Writes ReplicaSet to API Server â†’ etcd
```

**5. ReplicaSet Controller processing:**
```
ReplicaSet Controller:
1. Watch detects new ReplicaSet
2. Reads: replicas=3, no Pods exist
3. Creates 3 Pods:
   - day-service-7d4f9c8b5f-abc12
   - day-service-7d4f9c8b5f-def34
   - day-service-7d4f9c8b5f-ghi56
4. Writes Pods to API Server â†’ etcd
```

**6. Scheduler processing:**
```
Scheduler:
1. Watch detects 3 new Pods (nodeName: empty)
2. For each Pod:
   a. Filter nodes (CPU/memory available?)
   b. Score nodes (best fit?)
   c. Select best node
   d. Update Pod: nodeName=ip-10-1-1-50
3. Writes updates to API Server â†’ etcd
```

**7. Kubelet processing (on each node):**
```
Kubelet on ip-10-1-1-50 (EC2 instance):
1. Watch detects Pod assigned to this node
2. Calls CNI plugin:
   - Allocates IP from VPC subnet: 10.1.1.101
   - Sets up network namespace
3. Calls Container Runtime (containerd):
   - Pulls image from ECR (AWS authentication)
   - Creates container from image
   - Mounts volumes (if any)
   - Applies resource limits
   - Starts container
4. Runs health probes (if defined)
5. Reports status to API Server:
   - Phase: Running
   - Ready: true
   - IP: 10.1.1.101
```

**8. Service creation (expose app):**
```yaml
apiVersion: v1
kind: Service
metadata:
  name: day-service
  namespace: production
spec:
  type: ClusterIP
  selector:
    app: day
  ports:
  - port: 80
    targetPort: 8001
```

**9. Service Controller & kube-proxy:**
```
Service Controller:
1. Detects new Service
2. Allocates ClusterIP: 10.100.200.50
3. Creates Endpoints:
   - 10.1.1.101:8001 (Pod 1)
   - 10.1.1.102:8001 (Pod 2)
   - 10.1.1.103:8001 (Pod 3)

kube-proxy (on every node):
1. Detects new Service
2. Creates iptables rules:
   DNAT: 10.100.200.50:80 â†’ random(10.1.1.101:8001, 10.1.1.102:8001, 10.1.1.103:8001)
```

**10. Ingress creation (external access):**
```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: day-ingress
  annotations:
    alb.ingress.kubernetes.io/scheme: internet-facing
spec:
  ingressClassName: alb
  rules:
  - host: day.example.com
    http:
      paths:
      - path: /
        backend:
          service:
            name: day-service
            port:
              number: 80
```

**11. AWS Load Balancer Controller:**
```
ALB Controller (running as Pod in cluster):
1. Detects new Ingress
2. Calls AWS APIs:
   - CreateLoadBalancer (ALB)
   - CreateTargetGroup (pod IPs directly!)
   - RegisterTargets:
     - 10.1.1.101:8001
     - 10.1.1.102:8001
     - 10.1.1.103:8001
   - CreateListener (port 80)
   - CreateRule (host: day.example.com â†’ target group)
3. Waits for ALB to become active
4. Updates Ingress status:
   loadBalancer.ingress[0].hostname: k8s-production-dayingre-xxx.us-east-1.elb.amazonaws.com
```

**12. Traffic flow (user request):**
```
User: curl http://day.example.com/health
    â†“
DNS: day.example.com â†’ k8s-production-dayingre-xxx.us-east-1.elb.amazonaws.com
    â†“
AWS ALB: k8s-production-dayingre-xxx.us-east-1.elb.amazonaws.com
    â†“ ALB chooses target (Pod IP directly - no kube-proxy!)
Pod: 10.1.1.102:8001
    â†“
Container: day:v1.2.3
    â†“
Flask app: return {"status": "healthy"}
    â†“
User receives: {"status": "healthy"}
```

---

## Real-World Example from This Project

Our Day cluster demonstrates all these concepts:

### Infrastructure Stack (Pulumi)
```python
# foundation/provisioning/pulumi/__main__.py

# 1. VPC (infrastructure layer)
vpc = aws.ec2.Vpc("day-vpc", cidr_block="10.1.0.0/16")

# 2. EKS Cluster (cluster layer)
cluster = eks.Cluster(
    "day-cluster",
    vpc_id=vpc.id,
    subnet_ids=[subnet.id for subnet in subnets],
)

# 3. Node Group (compute)
node_group = aws.eks.NodeGroup(
    "day-nodes",
    cluster_name=cluster.eks_cluster.name,
    instance_types=["t3.small"],
    scaling_config={
        "desired_size": 2,
        "min_size": 1,
        "max_size": 3,
    },
    capacity_type="SPOT",  # Using spot instances
)

# 4. ALB Controller (platform layer)
alb_controller = k8s.helm.v3.Release(
    "aws-load-balancer-controller",
    chart="aws-load-balancer-controller",
    repository_opts={"repo": "https://aws.github.io/eks-charts"},
)
```

### Application Stack (Pulumi or YAML)
```python
# foundation/gitops/pulumi_deploy/__main__.py

# Reference infrastructure stack
infra = pulumi.StackReference("organization/infrastructure/day")
kubeconfig = infra.get_output("kubeconfig")

# Create Kubernetes resources
deployment = k8s.apps.v1.Deployment(...)
service = k8s.core.v1.Service(...)
configmap = k8s.core.v1.ConfigMap(...)
hpa = k8s.autoscaling.v2.HorizontalPodAutoscaler(...)
ingress = k8s.networking.v1.Ingress(...)
```

### What Happens When We Deploy

```
1. Pulumi infrastructure stack:
   - Creates VPC in AWS
   - Creates EKS cluster (AWS manages control plane)
   - Creates node group (EC2 instances join cluster)
   - Installs ALB controller (Helm chart â†’ Kubernetes Deployment)

2. Pulumi application stack:
   - Connects to EKS via kubeconfig
   - Creates Deployment â†’ ReplicaSet â†’ Pods
   - Scheduler assigns Pods to nodes
   - Kubelet starts containers
   - Creates Service (ClusterIP)
   - kube-proxy configures iptables
   - Creates Ingress
   - ALB Controller creates AWS ALB
   - Traffic flows: Internet â†’ ALB â†’ Pods

3. Autoscaling:
   - HPA watches CPU/memory metrics
   - Scales replicas: 2 â†’ 5 (if high load)
   - Cluster Autoscaler adds nodes if needed
   - AWS Auto Scaling Group launches EC2 instances

4. Updates:
   - Change image: day:v1.2.3 â†’ day:v1.2.4
   - Deployment Controller creates new ReplicaSet
   - Rolling update: old pods â†’ new pods
   - Zero downtime!
```

---

## Summary

### Core Concepts
- **Pods** - Smallest deployable unit (wraps containers)
- **Deployments** - Manage Pods, handle updates/scaling
- **Services** - Stable network endpoint for Pods
- **ConfigMaps/Secrets** - Configuration management
- **Ingress** - HTTP routing to Services

### Architecture
- **Control Plane** - Brain (API Server, etcd, Scheduler, Controllers)
- **Data Plane** - Muscle (Nodes, Kubelet, Container Runtime)
- **Declarative** - Desired state â†’ Controllers reconcile

### Cloud Integration
- **Cloud Controller Manager** - Interfaces with cloud APIs
- **LoadBalancer Services** - Create cloud load balancers
- **CSI** - Provision cloud storage (EBS, EFS)
- **CNI** - Cloud networking (VPC integration)
- **IRSA** - Cloud identity (IAM roles for Pods)

### EKS Specifics
- **AWS manages** - Control plane, etcd, upgrades
- **You manage** - Nodes, applications, add-ons
- **Fully compatible** - Standard Kubernetes APIs
- **AWS integrated** - VPC CNI, ALB Controller, EBS CSI

### The Flow
```
Developer â†’ kubectl â†’ API Server â†’ etcd
                         â†“
            Controllers watch â†’ Reconcile
                         â†“
                   Scheduler â†’ Assign Pods
                         â†“
                   Kubelet â†’ Start Containers
                         â†“
                 Cloud APIs â†’ Provision Resources
                         â†“
                  Running Application!
```

## Next Steps

**Explore deeper:**
- [deployment-hierarchy.md](../05-kubernetes-deep-dives/deployment-hierarchy.md) - How Deployments work
- [configmap-relationships.md](../05-kubernetes-deep-dives/configmap-relationships.md) - Configuration management
- [rolling-updates.md](../05-kubernetes-deep-dives/rolling-updates.md) - Zero-downtime updates
- [two-tier-architecture.md](../02-infrastructure-as-code/two-tier-architecture.md) - Infrastructure as Code

**Try it yourself:**
- [first-deployment.md](first-deployment.md) - Deploy your first cluster
- [deploy-with-pulumi.md](../02-infrastructure-as-code/deploy-with-pulumi.md) - Pulumi-managed cluster
- Exploration scripts: `foundation/gitops/manual_deploy/explore/`

**Official documentation:**
- [Kubernetes Documentation](https://kubernetes.io/docs/)
- [EKS Documentation](https://docs.aws.amazon.com/eks/)
- [Kubernetes The Hard Way](https://github.com/kelseyhightower/kubernetes-the-hard-way)

---

You now understand how Kubernetes works from containers to cloud! ğŸš€
